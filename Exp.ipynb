{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "028e6657",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import argparse\n",
    "import pickle\n",
    "import transformers\n",
    "import mlflow\n",
    "import logging\n",
    "import pdb\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "import helpers\n",
    "import models\n",
    "import lossFunc\n",
    "from train_hyper import train_hyper\n",
    "from spl_utills import *\n",
    "\n",
    "global_step = 0\n",
    "moving_weights_all = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fbbf6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_train(model, train_loader, val_loader, args):\n",
    "    lr = args.pre_lr\n",
    "    epochs = args.pre_epochs\n",
    "    n_warmup = args.pre_n_warmup\n",
    "    if args.pre_optim == 'adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=args.pre_wd)\n",
    "    elif args.pre_optim == 'sgd':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.8, weight_decay=args.pre_wd)\n",
    "    else:\n",
    "        raise ValueError(f'Invalid optimizer name {args.pre_optim}')\n",
    "    if args.pre_cos:\n",
    "        scheduler = transformers.get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=n_warmup,\n",
    "                                                                 num_training_steps=epochs)\n",
    "    else:\n",
    "        scheduler = transformers.get_constant_schedule_with_warmup(optimizer, num_warmup_steps=n_warmup)\n",
    "\n",
    "    history_loss = []\n",
    "    history_acc = []\n",
    "\n",
    "    best_val_model = copy.deepcopy(model.state_dict())\n",
    "    best_val_acc = 0.\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        loss_meter = helpers.AverageMeter()\n",
    "        pu_acc_meter = helpers.AverageMeter()\n",
    "        pn_acc_meter = helpers.AverageMeter()\n",
    "        model.train()\n",
    "        for data, labels, true_labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            if args.cuda:\n",
    "                data, labels, true_labels = data.cuda(), labels.cuda(), true_labels.cuda()\n",
    "\n",
    "            net_out = model(data)\n",
    "\n",
    "            if args.pre_loss == 'bce':\n",
    "                loss = lossFunc.bce_loss(net_out, labels)\n",
    "            else:\n",
    "                loss = getattr(lossFunc, f'{args.pre_loss}_loss')(net_out, labels, args.prior, sur_loss=args.sur_loss)\n",
    "            pu_acc = Metrics.accuracy(net_out, labels)\n",
    "            pn_acc = Metrics.accuracy(net_out, true_labels)\n",
    "\n",
    "            loss_meter.update(loss.item(), data.size(0))\n",
    "            pu_acc_meter.update(pu_acc, data.size(0))\n",
    "            pn_acc_meter.update(pn_acc, data.size(0))\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        val_loss, val_acc = test(model, val_loader, args)\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_val_model = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        print(\n",
    "            f'Pre-Epoch [{epoch + 1} / {epochs}]  Loss: {loss_meter.avg:.5f}    PU-Acc: {pu_acc_meter.avg * 100.0:.5f}  PN-ACC: {pn_acc_meter.avg * 100.0:.5f}    val_loss: {val_loss:.5f}  val_acc: {val_acc * 100.0:.5f}')\n",
    "\n",
    "        history_loss.append(loss_meter.avg)\n",
    "        history_acc.append(pu_acc_meter.avg)\n",
    "\n",
    "    history = {'loss': history_loss, 'acc': history_acc}\n",
    "    model.load_state_dict(best_val_model)\n",
    "\n",
    "    if not os.path.exists('models'):\n",
    "        os.makedirs('models')\n",
    "    torch.save(model.state_dict(),\n",
    "               f'models/{args.dataset}_{args.prior}_{lr}_{epochs}_{args.pre_loss}_{args.pre_batch_size}.pth')\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16a7e929",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, loader, args):\n",
    "    training = model.training\n",
    "    model.eval()\n",
    "    loss_meter = helpers.AverageMeter()\n",
    "    acc_meter = helpers.AverageMeter()\n",
    "    with torch.no_grad():\n",
    "        for data, labels in loader:\n",
    "            if args.cuda:\n",
    "                data, labels = data.cuda(), labels.cuda()\n",
    "            net_out = model(data)\n",
    "            loss = lossFunc.bce_loss(net_out, labels)\n",
    "            acc = Metrics.accuracy(net_out, labels)\n",
    "\n",
    "            loss_meter.update(loss.item(), data.size(0))\n",
    "            acc_meter.update(acc, data.size(0))\n",
    "\n",
    "    model.train(training)\n",
    "    return loss_meter.avg, acc_meter.avg\n",
    "\n",
    "\n",
    "def get_fea(model, dataloader, args):\n",
    "    training = model.training\n",
    "    model.eval()\n",
    "    fea_all = []\n",
    "    true_labels_all = []\n",
    "    labels_all = []\n",
    "    with torch.no_grad():\n",
    "        for data, labels, true_labels in dataloader:\n",
    "            if args.cuda:\n",
    "                data, labels, true_labels = data.cuda(), labels.cuda(), true_labels.cuda()\n",
    "\n",
    "            net_out, fea = model(data, return_fea=True)\n",
    "            fea_all.append(fea.cpu().numpy())\n",
    "            labels_all.append(labels.cpu().numpy())\n",
    "            true_labels_all.append(true_labels.cpu().numpy())\n",
    "        fea_all = np.concatenate(fea_all, axis=0)\n",
    "        labels_all = np.concatenate(labels_all, axis=0)\n",
    "        true_labels_all = np.concatenate(true_labels_all, axis=0)\n",
    "    model.train(training)\n",
    "    return fea_all, labels_all, true_labels_all\n",
    "\n",
    "\n",
    "def weighted_dataloader(model, dataloader, thresh, args):\n",
    "    # calculate weights for all\n",
    "    training = model.training\n",
    "    model.eval()\n",
    "    data_all, labels_all, true_labels_all, weights_all, probs_all = [], [], [], [], []\n",
    "    global moving_weights_all\n",
    "    with torch.no_grad():\n",
    "        for data, labels, true_labels in dataloader:\n",
    "            if args.cuda:\n",
    "                data, labels, true_labels = data.cuda(), labels.cuda(), true_labels.cuda()\n",
    "            net_out = model(data)\n",
    "\n",
    "            # unlabeled data with linear weight\n",
    "            probs = torch.sigmoid(net_out)\n",
    "            probs_all.append(probs)\n",
    "            # loss for calculating weight\n",
    "            loss = lossFunc.logistic_loss(net_out / args.temper, -1)\n",
    "            weights = calculate_spl_weights(loss.detach(), thresh, args)\n",
    "            # positive data with weight=1.0\n",
    "            weights[labels == 1] = 1.\n",
    "\n",
    "            data_all.append(data)\n",
    "            labels_all.append(labels)\n",
    "            true_labels_all.append(true_labels)\n",
    "            weights_all.append(weights)\n",
    "\n",
    "        data_all = torch.cat(data_all, dim=0)\n",
    "        labels_all = torch.cat(labels_all, dim=0)\n",
    "        true_labels_all = torch.cat(true_labels_all, dim=0)\n",
    "        weights_all = torch.cat(weights_all, dim=0)\n",
    "        if moving_weights_all is None:\n",
    "            moving_weights_all = weights_all\n",
    "        else:\n",
    "            moving_weights_all = args.phi * moving_weights_all + (1. - args.phi) * weights_all\n",
    "        probs_all = torch.cat(probs_all, dim=0)\n",
    "\n",
    "        unlabel_weights = moving_weights_all[labels_all == -1]\n",
    "        unlabel_true_labels = true_labels_all[labels_all == -1]\n",
    "        unlabel_probs = probs_all[labels_all == -1]\n",
    "        logging.info(\n",
    "            f'Mean weight of positive-unlabeled: {unlabel_weights[unlabel_true_labels == 1].mean()}\\tMean weight of negative-unlabeled: {unlabel_weights[unlabel_true_labels == -1].mean()}')\n",
    "        logging.info(\n",
    "            f'Mean probability of positive-unlabeled: {unlabel_probs[unlabel_true_labels == 1].mean()}\\tMean probability of negative-unlabeled: {unlabel_probs[unlabel_true_labels == -1].mean()}')\n",
    "\n",
    "    dataloader = DataLoader(TensorDataset(data_all, labels_all, true_labels_all, moving_weights_all), shuffle=True,\n",
    "                            batch_size=args.batch_size)\n",
    "    model.train(training)\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "def train_episode(model, dataloader, args):\n",
    "    if args.optim == 'adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.wd)\n",
    "    else:\n",
    "        optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=0.9, weight_decay=args.wd)\n",
    "    if args.cos:\n",
    "        scheduler = transformers.get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=args.n_warmup,\n",
    "                                                                 num_training_steps=args.inner_epochs)\n",
    "    else:\n",
    "        scheduler = transformers.get_constant_schedule_with_warmup(optimizer, num_warmup_steps=args.n_warmup)\n",
    "\n",
    "    # Test on training set before training\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        meter = helpers.AverageMeter()\n",
    "        for data, labels, true_labels, weights in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            if args.cuda:\n",
    "                data, labels, true_labels, weights = data.cuda(), labels.cuda(), true_labels.cuda(), weights.cuda()\n",
    "\n",
    "            net_out = model(data)\n",
    "\n",
    "            # loss w.r.t. pseudo labels\n",
    "            if args.loss == 'bce':\n",
    "                loss = lossFunc.bce_loss(net_out, labels, weights)\n",
    "            else:\n",
    "                loss = getattr(lossFunc, f'{args.loss}_loss')(net_out, labels, args.prior, weights, sur_loss=args.sur_loss)\n",
    "            meter.update(loss.item(), labels.size(0))\n",
    "        logging.info(f'Loss before training: {meter.avg}')\n",
    "\n",
    "    if args.restart:\n",
    "        model.reset_para()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    tot_loss_meter = helpers.AverageMeter()\n",
    "    tot_true_loss_meter = helpers.AverageMeter()\n",
    "    tot_acc_meter = helpers.AverageMeter()\n",
    "    for inner_epoch in range(args.inner_epochs):\n",
    "        loss_meter = helpers.AverageMeter()\n",
    "        for data, labels, true_labels, weights in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            if args.cuda:\n",
    "                data, labels, true_labels, weights = data.cuda(), labels.cuda(), true_labels.cuda(), weights.cuda()\n",
    "\n",
    "            net_out = model(data)\n",
    "\n",
    "            # loss w.r.t. pseudo labels\n",
    "            if args.loss == 'bce':\n",
    "                loss = lossFunc.bce_loss(net_out, labels, weights, do_norm=False)\n",
    "            else:\n",
    "                loss = getattr(lossFunc, f'{args.loss}_loss')(net_out, labels, args.prior, weights, sur_loss=args.sur_loss)\n",
    "            loss += args.w_entropy * lossFunc.entropy_loss(net_out)\n",
    "            # loss w.r.t. true labels\n",
    "            true_loss = lossFunc.bce_loss(net_out, true_labels, weights)\n",
    "            # acc w.r.t. true labels\n",
    "            acc = Metrics.accuracy(net_out, true_labels)\n",
    "\n",
    "            tot_loss_meter.update(loss.item(), data.size(0))\n",
    "            tot_acc_meter.update(acc, data.size(0))\n",
    "            tot_true_loss_meter.update(true_loss.item(), data.size(0))\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_meter.update(loss.item(), labels.size(0))\n",
    "\n",
    "        scheduler.step()\n",
    "        logging.debug(f'inner epoch [{inner_epoch + 1} / {args.inner_epochs}]  train loss: {loss_meter.avg}')\n",
    "\n",
    "        if args.debug:\n",
    "            global global_step\n",
    "            mlflow.log_metric('train_loss', loss_meter.avg, global_step)\n",
    "            global_step += 1\n",
    "\n",
    "    return tot_loss_meter.avg, tot_acc_meter.avg, tot_true_loss_meter.avg\n",
    "\n",
    "\n",
    "def train(model, positive_dataset, unlabeled_dataset, val_dataset, test_dataset, args):\n",
    "    val_loader = DataLoader(val_dataset, batch_size=args.batch_size_val, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=args.batch_size_val, shuffle=False)\n",
    "\n",
    "    epochs = args.epochs\n",
    "    batch_size = args.batch_size\n",
    "    patience = args.patience\n",
    "    positive_data, positive_labels = positive_dataset.X, positive_dataset.y\n",
    "    unlabeled_data, unlabeled_labels = unlabeled_dataset.X, unlabeled_dataset.y\n",
    "\n",
    "    # for SPL\n",
    "    cl_scheduler = TrainingScheduler(args.scheduler_type, args.alpha, args.max_thresh, args.grow_steps, args.p,\n",
    "                                     args.eta)\n",
    "\n",
    "    history_loss = []\n",
    "    history_acc = []\n",
    "    history_true_loss = []\n",
    "    history_val_loss = []\n",
    "    history_val_acc = []\n",
    "\n",
    "    val_best_acc = 0.\n",
    "    val_best_index = -1\n",
    "    val_best_model = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    fea_all = []\n",
    "\n",
    "    for episode in range(epochs):\n",
    "        # get next lambda\n",
    "        thresh = cl_scheduler.get_next_ratio()\n",
    "        helpers.prRedWhite(f'thresh = {thresh:.3f}')\n",
    "        cur_data = torch.cat((positive_data, unlabeled_data), dim=0)\n",
    "        cur_labels = torch.cat((positive_labels, -torch.ones_like(unlabeled_labels)), dim=0)\n",
    "        cur_true_labels = torch.cat((positive_labels, unlabeled_labels), dim=0)\n",
    "        perm = np.random.permutation(cur_data.size(0))\n",
    "        cur_data, cur_labels, cur_true_labels = cur_data[perm], cur_labels[perm], cur_true_labels[perm]\n",
    "        cur_loader = DataLoader(TensorDataset(cur_data, cur_labels, cur_true_labels), batch_size=batch_size,\n",
    "                                shuffle=True)\n",
    "        weighted_loader = weighted_dataloader(model, cur_loader, thresh, args)\n",
    "\n",
    "        if args.vis and episode == 0:\n",
    "            fea_all.append(get_fea(model, cur_loader, args))\n",
    "\n",
    "        tot_loss, tot_acc, tot_true_loss = train_episode(model, weighted_loader, args)\n",
    "\n",
    "        if args.vis:\n",
    "            fea_all.append(get_fea(model, cur_loader, args))\n",
    "\n",
    "        val_loss, val_acc = test(model, val_loader, args)\n",
    "        test_loss, test_acc = test(model, test_loader, args)\n",
    "        print(\n",
    "            f'Episode [{episode + 1} / {epochs}]   Pseudo_Loss: {tot_loss:.5f}  True_Loss: {tot_true_loss:.5f}  True_Acc: {tot_acc * 100.0:.5f}   val_loss: {val_loss:.5f}  val_acc: {val_acc * 100.0:.5f}    test_loss: {test_loss: .5f}  test_acc: {test_acc * 100.0:.5f}')\n",
    "\n",
    "        history_loss.append(tot_loss)\n",
    "        history_acc.append(tot_acc)\n",
    "        history_true_loss.append(tot_true_loss)\n",
    "        history_val_loss.append(val_loss)\n",
    "        history_val_acc.append(val_acc)\n",
    "\n",
    "        if args.debug:\n",
    "            mlflow.log_metric('val_loss', val_loss)\n",
    "            mlflow.log_metric('val_err', 100.0 - val_acc * 100.0)\n",
    "\n",
    "        # Early stop\n",
    "        if val_acc > val_best_acc:\n",
    "            val_best_acc = val_acc\n",
    "            val_best_index = episode\n",
    "            val_best_model = copy.deepcopy(model.state_dict())\n",
    "        else:\n",
    "            if episode - val_best_index >= patience:\n",
    "                print(f'=== Break at epoch {val_best_index + 1} ===')\n",
    "                fea_all = fea_all[:val_best_index + 2]\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(val_best_model)\n",
    "\n",
    "    if args.vis:\n",
    "        if not os.path.exists('data_anal'):\n",
    "            os.mkdir('data_anal')\n",
    "        pickle.dump(fea_all, open(f'data_anal/{args.dataset}_{args.prior}.npy', 'wb'))\n",
    "\n",
    "    history = {'pseudo_loss': history_loss, 'true_loss': history_true_loss, 'acc': history_acc,\n",
    "               'val_loss': history_val_loss, 'val_acc': history_val_acc}\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b591efd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_and_run(args):\n",
    "    seed_all(args.seed)\n",
    "#     pdb.set_trace()\n",
    "    positive_dataset, unlabeled_dataset, pretrain_dataset, val_dataset, test_dataset, input_size = get_datasets(\n",
    "        args.dataset,\n",
    "        args.n_labeled,\n",
    "        args.n_unlabeled,\n",
    "        args.prior,\n",
    "        root=args.data_dir,\n",
    "        n_valid=args.n_valid,\n",
    "        n_test=args.n_test,\n",
    "        return_pretrain=True)\n",
    "    args.input_size = input_size\n",
    "    \n",
    "    model = getattr(models, args.model)(args.input_size)\n",
    "    # if args.cuda:\n",
    "    #     model = model.cuda()\n",
    "    pdb.set_trace()\n",
    "    if args.dataset == 'risk':\n",
    "        model.train()\n",
    "        train_loader = DataLoader(unlabeled_dataset, batch_size=args.batch_size_val, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=args.batch_size_val, shuffle=False)\n",
    "        cl_scheduler = TrainingScheduler(args.scheduler_type, args.alpha, args.max_thresh, args.grow_steps, args.p,\n",
    "                                         args.eta)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.wd)\n",
    "        for epi in range(10):\n",
    "            thresh = cl_scheduler.get_next_ratio()\n",
    "            print(f'------- thresh: {thresh} ------------')\n",
    "            for epo in range(20):\n",
    "                model.train()\n",
    "                for data, labels in train_loader:\n",
    "                    # data, labels = data.cuda(), labels.cuda()\n",
    "                    optimizer.zero_grad()\n",
    "                    # data, label = data.cuda(), labels.cuda()\n",
    "                    net_out = model(data).squeeze()\n",
    "                    loss = lossFunc.bce_loss(net_out, labels)\n",
    "                    weight = calculate_spl_weights(loss.detach(), thresh, args)\n",
    "                    loss = lossFunc.bce_loss(net_out, labels, weight)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                model.eval()\n",
    "                acc_m = helpers.AverageMeter()\n",
    "                with torch.no_grad():\n",
    "                    for data, labels in test_loader:\n",
    "                        # data, labels = data.cuda(), labels.cuda()\n",
    "                        net_out = model(data).squeeze()\n",
    "                        labels[labels == -1] = 0\n",
    "                        acc = Metrics.accu(net_out, labels)\n",
    "                        acc_m.update(acc, len(data))\n",
    "                print(f'acc: {acc_m.avg}')\n",
    "\n",
    "    if args.pretrained:\n",
    "        print(f'Model loaded from: {args.pretrained}.')\n",
    "        model.load_state_dict(torch.load(args.pretrained))\n",
    "    else:\n",
    "        val_loader = DataLoader(val_dataset, batch_size=args.batch_size_val, shuffle=False)\n",
    "        pretrain_loader = DataLoader(pretrain_dataset, batch_size=args.pre_batch_size, shuffle=True)\n",
    "        pre_train(model, pretrain_loader, val_loader, args)\n",
    "\n",
    "    seed_all(args.seed)\n",
    "\n",
    "    # Test before train\n",
    "    val_loader = DataLoader(val_dataset, batch_size=args.batch_size_val, shuffle=False)\n",
    "    val_loss, val_acc = test(model, val_loader, args)\n",
    "    un_loader = DataLoader(unlabeled_dataset, batch_size=args.batch_size_val, shuffle=False)\n",
    "    un_loss, un_acc = test(model, un_loader, args)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=args.batch_size_val, shuffle=False)\n",
    "    test_loss, test_acc = test(model, test_loader, args)\n",
    "    helpers.prYellow(\n",
    "        f'Before training  un-Loss: {un_loss:.5f}  un-Acc: {un_acc * 100.0: .5f}    val-Loss: {val_loss:.5f}  val-Acc: {val_acc * 100.0: .5f}   Test-Acc: {test_acc * 100.0:.5f}')\n",
    "\n",
    "    if args.bilevel:\n",
    "        history = train_hyper(model, positive_dataset, unlabeled_dataset, val_dataset, args)\n",
    "    else:\n",
    "        history = train(model, positive_dataset, unlabeled_dataset, val_dataset, test_dataset, args)\n",
    "\n",
    "    test_loader = DataLoader(test_dataset, batch_size=args.batch_size_val, shuffle=False)\n",
    "    test_loss, test_acc = test(model, test_loader, args)\n",
    "    test_err = 1. - test_acc\n",
    "    print(f'Test  Loss: {test_loss}   Error: {100.0 * test_err}')\n",
    "\n",
    "    return history, test_loss, test_err\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a682bf3",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset exp loading.\n",
      "labeled: 561    unlabeled: 0    unlabeled positive: 0    unlabeled negative: 0    pos_targets: [1]    prior: 0.84\n",
      "labeled: 0    unlabeled: 100    unlabeled positive: 84    unlabeled negative: 16    pos_targets: [1]    prior: 0.84\n",
      "labeled: 0    unlabeled: 200    unlabeled positive: 168    unlabeled negative: 32    pos_targets: [1]    prior: 0.84\n",
      "input_size: 1343\n",
      "> \u001b[1;32mc:\\users\\lenovo\\appdata\\local\\temp\\ipykernel_106144\\330201803.py\u001b[0m(19)\u001b[0;36mprepare_and_run\u001b[1;34m()\u001b[0m\n",
      "\n",
      "ipdb> q\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_106144/2930855020.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    159\u001b[0m     \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_argument\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'--phi'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhelp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'momentum for weight moving average (default: 0.)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[1;31m#     print(parser.parse_args())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 161\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparser\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_106144/2930855020.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m(parser)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[0mprepare_and_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_106144/330201803.py\u001b[0m in \u001b[0;36mprepare_and_run\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;31m#     model = model.cuda()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mpdb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'risk'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mtrain_loader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munlabeled_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_106144/330201803.py\u001b[0m in \u001b[0;36mprepare_and_run\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;31m#     model = model.cuda()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mpdb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'risk'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mtrain_loader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munlabeled_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[1;34m(self, frame, event, arg)\u001b[0m\n\u001b[0;32m     86\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;31m# None\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'line'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'call'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\lib\\bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[1;34m(self, frame)\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main(parser):\n",
    "#     pdb.set_trace()\n",
    "    args = parser.parse_args(args=['--dataset','exp','--data_dir','data'])\n",
    "    args.cuda = args.no_cuda\n",
    "    logging.basicConfig(level=(logging.DEBUG if args.debug else logging.INFO))\n",
    "    if args.run_all:\n",
    "        args.debug = False\n",
    "\n",
    "    if args.dataset == 'cifar10':\n",
    "        args.model = 'CNN'\n",
    "        args.n_labeled = 2000\n",
    "        args.n_unlabeled = 4000\n",
    "        args.n_valid = 500\n",
    "        args.n_test = 5000\n",
    "    elif args.dataset == 'mnist':\n",
    "        args.model = 'normalNN'\n",
    "        args.n_labeled = 2000\n",
    "        args.n_unlabeled = 4000\n",
    "        args.n_valid = 500\n",
    "        args.n_test = 5000\n",
    "    # for UCI datasets\n",
    "    else:\n",
    "        args.model = 'normalNN'\n",
    "        args.n_labeled = 561\n",
    "        args.n_unlabeled = 0\n",
    "        args.n_valid = 100\n",
    "        args.n_test = 200\n",
    "        args.prior = 0.84\n",
    "\n",
    "    if args.cuda:\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(args.gpu_id)\n",
    "    \n",
    "    if args.run_all:\n",
    "        for prior in [0.2, 0.4, 0.6]:\n",
    "            args.prior = prior\n",
    "            test_losses = []\n",
    "            test_errors = []\n",
    "\n",
    "            for seed in range(10):\n",
    "                args.seed = seed\n",
    "                history, test_loss, test_err = prepare_and_run(args)\n",
    "\n",
    "                test_losses.append(test_loss)\n",
    "                test_errors.append(test_err)\n",
    "\n",
    "            test_losses = np.array(test_losses)\n",
    "            test_errors = np.array(test_errors)\n",
    "            print(f'Test    Loss: {test_losses.mean()}   Error: {test_errors.mean() * 100.0}')\n",
    "\n",
    "            mlflow.log_metric(f'test_err{prior}', test_errors.mean() * 100.0)\n",
    "\n",
    "            print(f'--- prior: {args.prior}   Error: {test_errors.mean() * 100.0:.3f} ({test_errors.std():.3f}) ---')\n",
    "\n",
    "    else:\n",
    "        prepare_and_run(args)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='PyTorch Implementation of CL+PU')\n",
    "\n",
    "    parser.add_argument('--debug', action='store_true')\n",
    "    parser.add_argument('--vis', action='store_true')\n",
    "    # Dirs\n",
    "    parser.add_argument(\"--output_dir\", type=str, default=os.getenv(\"AMLT_OUTPUT_DIR\", \"results\"),\n",
    "                        help='output dir (default: results)')\n",
    "    parser.add_argument(\"--data_dir\", type=str, default=os.listdir(os.getenv(\"AMLT_DATA_DIR\", \"./data\")),\n",
    "                        help=\"Directory where dataset is stored\")\n",
    "    # Dataset\n",
    "    parser.add_argument('--dataset', type=str, choices=['cifar10', 'exp', 'mushroom', 'shuttle', 'spambase', 'risk'],\n",
    "                        default='risk',\n",
    "                        help='dataset name (default: mnist)')\n",
    "    parser.add_argument('--seed', type=int, default=0, help='random seed (default: 0)')\n",
    "\n",
    "    parser.add_argument('--n_labeled', type=int, default=1000, help='number of positive samples (default: 1000)')\n",
    "    parser.add_argument('--n_unlabeled', type=int, default=0, help='number of unlabeled samples (default: 0)')\n",
    "    parser.add_argument('--n_valid', type=int, default=100, help='number of valid samples (default: 50)')\n",
    "    parser.add_argument('--n_test', type=int, default=200, help='number of valid samples (default: 150)')\n",
    "    parser.add_argument('--prior', type=float, default=0.2,\n",
    "                        help='ratio of unlabeled positive to unlabeled (default 0.2)')\n",
    "\n",
    "    # GPU\n",
    "    parser.add_argument('--no_cuda', action='store_true',\n",
    "                        help='disable cuda (default: False)')\n",
    "    parser.add_argument('--gpu_id', type=int, default=0,\n",
    "                        help='set gpu id to use (default: 0)')\n",
    "\n",
    "    # Pre-training\n",
    "    parser.add_argument('--pre_optim', type=str, default='adam', choices=['adam', 'sgd'], help='name of optimizer for pre-training (default adam)')\n",
    "    parser.add_argument('--pre_epochs', type=int, default=400,\n",
    "                        help='number of pre-training epochs (default 400)')\n",
    "    parser.add_argument('--pre_lr', type=float, default=1e-3, help='pre-training learning rate (default 1e-3)')\n",
    "    parser.add_argument('--pre_wd', type=float, default=0., help='weight decay for pre-training (default 0.)')\n",
    "    parser.add_argument('--pre_batch_size', type=int, default=64, help='batch size for pre-training (default 64)')\n",
    "    parser.add_argument('--pre_n_warmup', type=int, default=0,\n",
    "                        help='number of warm-up steps in pre-training (default 0)')\n",
    "    parser.add_argument('--pre_cos', action='store_true',\n",
    "                        help='Use cosine lr scheduler in pre-training (default False)')\n",
    "    parser.add_argument('--pretrained', type=str, default=None,\n",
    "                        help='pre-trained model path (default None)')\n",
    "\n",
    "    # Training\n",
    "    parser.add_argument('--epochs', type=int, default=100,\n",
    "                        help='number of training epochs to run (default: 100)')\n",
    "    parser.add_argument('--batch_size', default=64, type=int,\n",
    "                        help='mini-batch size (default: 64)')\n",
    "    parser.add_argument('--batch_size_val', default=128, type=int,\n",
    "                        help='mini-batch size of validation (default: 128)')\n",
    "    parser.add_argument('--optim', type=str, default='adam', help='type of optimizer for training (default: adam)')\n",
    "    parser.add_argument('--lr', default=1e-4, type=float,\n",
    "                        help='learning rate (default: 1e-4)')\n",
    "    parser.add_argument('--wd', default=0., type=float, help='weight decay (default 0.)')\n",
    "    parser.add_argument('--decay_epoch', default=-1, type=int,\n",
    "                        help='Reduces the learning rate every decay_epoch (default -1)')\n",
    "    parser.add_argument('--lr_decay', default=0.5, type=float,\n",
    "                        help='Learning rate decay for training (default: 0.5)')\n",
    "    parser.add_argument('--cos', action='store_true',\n",
    "                        help='Use cosine lr scheduler (default False)')\n",
    "    parser.add_argument('--n_warmup', default=0, type=int,\n",
    "                        help='Number of warm-up steps (default: 0)')\n",
    "    parser.add_argument('--patience', default=5, type=int, help='patience for early stopping (default 5)')\n",
    "    parser.add_argument('--w_entropy', default=0, type=float, help='weight of entropy loss (default 0)')\n",
    "    parser.add_argument('--restart', action='store_true', help='reset model before training in each episode (default: False)')\n",
    "\n",
    "    # Test\n",
    "    parser.add_argument('--run_all', action='store_true', help='run all experiences with 20 seeds (default False)')\n",
    "\n",
    "    # CL\n",
    "    parser.add_argument('--inner_epochs', type=int, default=1,\n",
    "                        help='number of epochs to run after each dataset update (default: 1)')\n",
    "    parser.add_argument('--max_thresh', type=float, default=2., help='maximum of threshold (default 2.0)')\n",
    "    parser.add_argument('--grow_steps', type=int, default=10, help='number of step to grow to max_thresh (default 10)')\n",
    "    parser.add_argument('--scheduler_type', type=str, default='exp',\n",
    "                        choices=['const', 'exp', 'linear', 'rootp', 'geom'],\n",
    "                        help='type of training scheduler (default exp)')\n",
    "    parser.add_argument('--alpha', type=float, default=0.1, help='initial threshold (default 0.1)')\n",
    "    parser.add_argument('--eta', type=float, default=1.1,\n",
    "                        help='alpha *= eta in each step for scheduler exp (default 1.1)')\n",
    "    parser.add_argument('--p', type=int, default=2, help='p for scheduler root-p (default 2)')\n",
    "    parser.add_argument('--spl_type', type=str, default='linear',\n",
    "                        choices=['hard', 'linear', 'log', 'mix2', 'logistic', 'poly', 'welsch', 'cauchy', 'huber', 'l1l2'],\n",
    "                        help='type of soft sp-regularizer (default linear)')\n",
    "    parser.add_argument('--mix2_gamma', type=float, default=1.0, help='gamma in mixture2 (default 1.0)')\n",
    "    parser.add_argument('--poly_t', type=int, default=3, help='t in polynomial (default 3)')\n",
    "\n",
    "    # BiLevel\n",
    "    parser.add_argument('--outer_iters', type=int, default=5, help='number of hyper-parameter updates (default: 5)')\n",
    "    parser.add_argument('--outer_lr', type=float, default=1e-2, help='lr for hyper-parameters (default: 1e-2)')\n",
    "    parser.add_argument('--hyper_K', type=int, default=10,\n",
    "                        help='the maximum number of conjugate gradient iterations (default: 10)')\n",
    "    parser.add_argument('--warm_start', action='store_true',\n",
    "                        help='whether to reuse params from previous hyper-iteration (default: False)')\n",
    "    parser.add_argument('--bilevel', action='store_true', help='enable bilevel for hyper-parameter optimize')\n",
    "\n",
    "    # PU\n",
    "    parser.add_argument('--pre_loss', type=str, default='bce', choices=['bce', 'nnpu', 'upu'])\n",
    "    parser.add_argument('--loss', type=str, default='bce')\n",
    "    parser.add_argument('--sur_loss', type=str, default='sigmoid')\n",
    "    parser.add_argument('--temper', type=float, default=1.0, help='temperature to smooth logits (default: 1.0)')\n",
    "    parser.add_argument('--phi', type=float, default=0., help='momentum for weight moving average (default: 0.)')\n",
    "#     print(parser.parse_args())\n",
    "    main(parser)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68e3343",
   "metadata": {},
   "source": [
    "# train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "950f388a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_data = pd.read_excel(\"../data1/SQLServerData/SampleData - CurtMain.xlsx\",sheet_name=\"LoanAllocation\")\n",
    "train_0_data = all_data[all_data[\"Approved\"]==0].iloc[36:]\n",
    "train_1_data = all_data[all_data[\"Approved\"]==1].iloc[164:]\n",
    "pd.concat([train_0_data,train_1_data],axis=0).to_csv(\"data/train_data.csv\")\n",
    "train_0_data = all_data[all_data[\"Approved\"]==0].iloc[:36]\n",
    "train_1_data = all_data[all_data[\"Approved\"]==1].iloc[:164]\n",
    "pd.concat([train_0_data,train_1_data],axis=0).to_csv(\"data/test_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ec9492",
   "metadata": {},
   "source": [
    "# LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7955ac0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset exp loading.\n",
      "0.84\n"
     ]
    }
   ],
   "source": [
    "train_data , train_label ,_ = preprocess_uci_dataset('exp','./data')\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_clf = LogisticRegression()\n",
    "lr_clf = lr_clf.fit(train_data[:800,],train_label[:800])\n",
    "test_acc = sum(lr_clf.predict(train_data[800:,])==train_label[800:])/200\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937199e3",
   "metadata": {},
   "source": [
    "# NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "24430f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset exp loading.\n"
     ]
    }
   ],
   "source": [
    "# 数据增强\n",
    "train_data , train_label ,_ = preprocess_uci_dataset('exp','./data')\n",
    "train_data = np.delete(train_data,list(range(270,614,1))+list(range(1200,1342,1)),1)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "train_data = scaler.fit_transform(train_data)\n",
    "train_data__ = np.vstack([train_data[:800,],np.repeat(train_data[np.where(train_label[:800]==0)],3,axis=0)])\n",
    "train_label__ =  np.hstack([train_label[:800,],np.zeros(158*3)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "20e71c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class normalNN(nn.Module):\n",
    "    def __init__(self, x_dim):\n",
    "        super(normalNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(x_dim, 64)\n",
    "        self.fc2 = nn.Linear(64,1)\n",
    "\n",
    "        self.af = nn.Sigmoid()\n",
    "\n",
    "        self.reset_para()\n",
    "\n",
    "    def forward(self, x, return_fea=False):\n",
    "        h = self.fc1(x)\n",
    "        h = self.af(h)\n",
    "        fea = h.detach().clone()\n",
    "        h = self.fc2(h)\n",
    "        h = self.af(h)\n",
    "        if return_fea:\n",
    "            return h.squeeze(), fea\n",
    "        return h.squeeze()\n",
    "\n",
    "    def reset_para(self):\n",
    "        # weight initialization\n",
    "        for m in self.modules():\n",
    "            if type(m) == nn.Linear or type(m) == nn.Conv2d or type(m) == nn.ConvTranspose2d:\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                if m.bias.data is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a61901a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\Desktop\\水木兼职\\12号\\new_code_spl\\utils.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.X = torch.tensor(X)\n",
      "C:\\Users\\lenovo\\Desktop\\水木兼职\\12号\\new_code_spl\\utils.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.y = torch.tensor(y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- thresh: 0.1 ------------\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "------- thresh: 0.11000000000000001 ------------\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "------- thresh: 0.12100000000000002 ------------\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "------- thresh: 0.13310000000000005 ------------\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "------- thresh: 0.14641000000000004 ------------\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "------- thresh: 0.16105100000000006 ------------\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "------- thresh: 0.1771561000000001 ------------\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "------- thresh: 0.19487171000000014 ------------\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "------- thresh: 0.21435888100000017 ------------\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "------- thresh: 0.2357947691000002 ------------\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n",
      "acc: 0.8399999737739563\n"
     ]
    }
   ],
   "source": [
    "args = parser.parse_args(args=['--dataset','exp','--data_dir','data','--lr','0.001','--wd','0.7','--spl_type','hard','--prior','0.8'])\n",
    "args.model = 'normalNN'\n",
    "args.n_labeled = 561\n",
    "args.n_unlabeled = 0\n",
    "args.n_valid = 100\n",
    "args.n_test = 200\n",
    "args.prior = 0.8\n",
    "        \n",
    "        \n",
    "train_dataset = PUDataset(torch.tensor(train_data__),torch.tensor(train_label__))\n",
    "test_dataset = PUDataset(torch.tensor(train_data[800:]),torch.tensor(train_label[800:]))\n",
    "args.input_size = np.prod(train_data[0].shape)\n",
    "model = normalNN(args.input_size)\n",
    "\n",
    "model.train()\n",
    "train_loader = DataLoader(train_dataset, batch_size=args.batch_size_val, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=args.batch_size_val, shuffle=False)\n",
    "cl_scheduler = TrainingScheduler(args.scheduler_type, args.alpha, args.max_thresh, args.grow_steps, args.p,\n",
    "                                 args.eta)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.wd)\n",
    "optimizer = optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.wd)\n",
    "for epi in range(10):\n",
    "    thresh = cl_scheduler.get_next_ratio()\n",
    "    print(f'------- thresh: {thresh} ------------')\n",
    "    for epo in range(20):\n",
    "        model.train()\n",
    "        for data, labels in train_loader:\n",
    "            # data, labels = data.cuda(), labels.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            # data, label = data.cuda(), labels.cuda()\n",
    "            net_out = model(data.float()).squeeze()\n",
    "            loss = lossFunc.bce_loss(net_out, labels)\n",
    "            weight = calculate_spl_weights(loss.detach(), 0.8, args)\n",
    "            loss = lossFunc.bce_loss(net_out, labels, weight)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        model.eval()\n",
    "        acc_m = helpers.AverageMeter()\n",
    "        with torch.no_grad():\n",
    "            for data, labels in test_loader:\n",
    "                # data, labels = data.cuda(), labels.cuda()\n",
    "                net_out = model(data.float()).squeeze()\n",
    "                labels[labels == -1] = 0\n",
    "                acc = Metrics.accu(net_out, labels)\n",
    "                acc_m.update(acc, len(data))\n",
    "        print(f'acc: {acc_m.avg}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
